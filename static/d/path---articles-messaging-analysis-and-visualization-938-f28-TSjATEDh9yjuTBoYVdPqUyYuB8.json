{"data":{"markdownRemark":{"html":"<p>About a year ago I attended a <a href=\"https://www.meetup.com/it-IT/NYC-D3-JS/events/236673854/\">Meetup on D3.js</a>. The first talk by <a href=\"https://github.com/mdezube\">Michael Dezube</a> was on using machine learning to analyze text messages and visualize the results. Today I want to walk through all the awesomeness we have in this project.</p>\n<h1 id=\"demo\"><a href=\"#demo\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Demo</h1>\n<p>The first chart we see is this\n\n  <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/sms-analysis-1-86b3d9848a6783f0fbfcf3250110969c-4b2e7.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n  \n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; ; max-width: 954px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 58.38574423480084%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsSAAALEgHS3X78AAADCklEQVQoz22T228UZRjG5x/wLzChQkhMiPGiNyboHd4ZMZh4Soh4p4VooiYYjamoCYhaBIytoIhFCgba7s52Zs/ttLRbemBpy7Z7ohR2ty3tlumed7a7M/PzY2uCF1788r7f8873JE/ebySwgcb/YgtMW2BtV5s6ll0X/XZt9lYdW8yqRo1TXV4k0ywLMUnDjDUxzUQTS2jYd4Vx4gm20IgLYoIoWNHtKma1WolvftSQLKskhPn/sCDMFthqTJGtaJyJTHH0xjTfjk2yXB7GeT/E2cgEnfPjjGVHkce9HDvp5EFGp+OvGaSiUeD28i1mV8LMZERNTZKrTFPaCpLQezk0rPHi3xqv9QaZ1/v4+paHNwJBjoz6OTHn4dC5a+x4+SLheJZT7ijSZEpn1/dBdn0XYOcxL3s+H2A6I1MwZVIFmXtFmdWyk42Kg0eGg0B6AG9KpWYpvD80zO4Lo7ReDDG7XqBjbBHpZloYdoVo+UnjmXYPu9v62P/uWT76uZv2cIC3j1/jzT9U2kaGuJpUODHr40jAx8HfFPZ0arzQM8wXoSD38nk6IymksfgyT39yiZaP+3j2M4XnvlJo2X+Ol9qv8N7IIPvae3n1vJsPRPTLCYVfYx6Oah72dqgcuBrg0xEfl5JeHlby9NxdQgpNRnlq5zvsaD1O6+Fe9vlCvDI4Ttu4xuk5N9eXVCK6jF51sFJ2kBOxkzmZ32Mq0U2ZmayTnoSXrJFjaEVEXni4xlvdV3j+B5XXL/tx3VfwpxUm1lzigktEcYptB8SSRsjXfER1BceSwpdTPq4vqoRWZSIbCuX6pvg+gZQp6fyS9HPAq/HhDb8Q+8Uy+smU+nlQdAjDfpZLHtYqQdJFN+HsAO6Uiz9F/K55FeeSS+guDHOT9UoMKVfdYGJV5eTtQU7fGWRuw81iTiW++fiym5trHu488hDTVcLrKrNiHhV9uqhwfiFId9xPqugRhjmqjaR42LZJzTTIbxkUBHWrSuNf6pZBrVGhVM03z1vmE+qCUt0QUY3m7PHvZ9qr/ANUpCOKkEoPCwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"texting per month\"\n        title=\"\"\n        src=\"/static/sms-analysis-1-86b3d9848a6783f0fbfcf3250110969c-4b2e7.png\"\n        srcset=\"/static/sms-analysis-1-86b3d9848a6783f0fbfcf3250110969c-37099.png 300w,\n/static/sms-analysis-1-86b3d9848a6783f0fbfcf3250110969c-f2478.png 600w,\n/static/sms-analysis-1-86b3d9848a6783f0fbfcf3250110969c-4b2e7.png 954w\"\n        sizes=\"(max-width: 954px) 100vw, 954px\"\n      />\n    </span>\n  </span>\n  \n  </a>\n    \nHere it shows us our messaging habits over the years. Fascinating how it varies! But let's dive deeper. The important question is: Who did we message? We can display the top N people:\n\n  <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/sms-analysis-2-b326bf83fada3d7ed8d7c9ae49b8d403-2fd0b.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n  \n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; ; max-width: 1165px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 58.11158798283261%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAAB70lEQVQoz52S/WvTQBzG89cKA/8BX2HDHzpxIr4UtcgclGq7DZ2jWsvYnMWxKaMwoTB12CR3yeU9sXEt2Hwf7zLWRhHfDj6575N8ee65y2nioAuxvwcnCOEIAcdx/hoh+13XxdGHI/R6PYRhCM1cqcJcvAMefwEzDDDGwDnP51N+p7nEYBy6riOKImhsrQ6jWskNuWnmzZZl5fMpf9K21Mo8jmOZ8MljeI8qsJOThMXGf8GUYfKE9rM6Wgs3cOhFEMz8LzOrmDBoNnD+7Cza7wV8e3pGE37WvzTkBcPnDVycmcO9VR2+ZYLZNkxLTA6cCwdMiOlCKk2eiP+g1ZZzw/DFMi7MzNH91T75gQ9z8yXCTpvcOCDuemCbLbg7WyRCqZVp/zM8bpAQFnEuf47eVzsjmZKiSCVcr9G5M7Mo12VjHJFVq9DSpXl0DuQ9iyJii7epfPk69j+6EGFIxsMyva3WcGh4sIKA9Ae3qFNr4BP3EctFtfTda9y8clesbVmjJE0Rv9nAtdLSqPnKzpJ0gHC7hVKpOmh3xDj5miLYWMf8fC3e3nXGyXEKt7mCqwuNdLfrZaPjATScjGWJwHR0JWlBtyRJQT+VDAp6RzJShUZyqEI9pzXFkm+qVHo4HO5lWTZZQH7zCTTG9IUyz1T5HUrXY/fhATJRAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"top people messaged with\"\n        title=\"\"\n        src=\"/static/sms-analysis-2-b326bf83fada3d7ed8d7c9ae49b8d403-2fd0b.png\"\n        srcset=\"/static/sms-analysis-2-b326bf83fada3d7ed8d7c9ae49b8d403-715e4.png 300w,\n/static/sms-analysis-2-b326bf83fada3d7ed8d7c9ae49b8d403-fcbdd.png 600w,\n/static/sms-analysis-2-b326bf83fada3d7ed8d7c9ae49b8d403-2fd0b.png 1165w\"\n        sizes=\"(max-width: 1165px) 100vw, 1165px\"\n      />\n    </span>\n  </span>\n  \n  </a>\n    \nBut who did we message and when?\n\n  <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/sms-analysis-3-10e5f2334114d2aaa588e029eb19ba12-2aea6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n  \n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; ; max-width: 1200px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 97.59229534510433%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAABYlAAAWJQFJUiTwAAADxklEQVQ4y3WVW28bRRTH/a34DnwFkHhBFQ8g8YCEuLwgHpAqRC/kYiBUpQgqktCSENIqUlEoTUkTJ46bxpvYsb3e2F57vd7Ye/He999zZtet04qxxjO7O/7tmfn/z3HOtm04joMgCGCaJizLwmg0wkDXMaTRHJyL50mSgNtknJ57nic6t1yj0cBeoYBSqYRi8YB6EdVaDZqmQVVVSPUa+n0dcRz/L5ADqFarcF0POf5R+aiMsiTh8PAQDVlGrJ/zarHY6DURBf4LgPi8Ei3vskucMIyQUxQFhmFgOBzCGBiwXBd+6RiRHwBhgs2lJRi0RgBElInozJoGyhQIbzt3dnaGKIow3Yrr/8GX2+hbLj5bm4PcLIv7vO0UhAz6Eqh2OqRDSBES0PfTLUVhCl5efoTh/W2ocPFVZRZFbT8FEiSMAwSxl205PVcWskPAMCRgvV4XEwHMznt+bg3avW3oMHG98i0e6lvIiHQKDkHNTBW8ANZISCFKu90WluGHIj7HxicfzGP3cRXdSMWV01lsnN3HgbQD1R7xAoyCnqC5kZNt2UKv1xNHl2NVJxGK772neO/SZVy+egf7D37DNfUmbp/8grw0iyPrGAVNQc9VECc9aN52CjQtsP2EKC2FztAZ04tdRB0VTz7/Hm/O5PHp5ixub11DvnUT+foNrFa+wV/6BnaG99BxH2IY78FO1jGmSLvuQHg25AhlOkPfssl7Q8RHVSwVfsUbj29hx1jF7uAnzNUWMK/S9cpHaCy+C6v4MTznLpyTH5HIC9DbdyD/uQhFacEjcXPsn3DaNsEBPnx6HdKoBme8h5nTeaxat6D9/Dbw5SVgM09ufwJIj5CUV5D88wc6b72PptxMgSx3IGxDvmIXRBauVr7DM7sFMiPWla8hJavAzDuINxaR9CQkJ2vC5EmQ2qe/sASNjismX+ZY7okocZJ642/1d0i2TNHqqPSuoItd4IcvKJq75D0bkLcuph5lGosyHrtppgjbCGBq1P3zLZw6Nbrho2UskxvbwMoNJAf/UmQkg/psKhUJSNWo2+2mxmYfRlF4wfmKXYLhnYq549XIn+SCnQdIzvuAS17U5QtAx3bQosBcL6s2ky1PgKavUBoOsh/5aTk4LiDxXSSOSWAtvZcBuY6qFKGohxzhdI1L2wivNW/8cp69eNK4OLRarRTIKnPp4qpsWWY6jgYinXgh56lD6WjSXFxn3aHOkY3HY1GMucgKIJO5cy6yUgxn5fmaX9ZoyBgMBqhWKmLk+qlQdun0F8FVmsdms0nAtGA8B9yx00GtLu5ZAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"people messaged over time\"\n        title=\"\"\n        src=\"/static/sms-analysis-3-10e5f2334114d2aaa588e029eb19ba12-abe1c.png\"\n        srcset=\"/static/sms-analysis-3-10e5f2334114d2aaa588e029eb19ba12-10046.png 300w,\n/static/sms-analysis-3-10e5f2334114d2aaa588e029eb19ba12-3c4a3.png 600w,\n/static/sms-analysis-3-10e5f2334114d2aaa588e029eb19ba12-abe1c.png 1200w,\n/static/sms-analysis-3-10e5f2334114d2aaa588e029eb19ba12-2aea6.png 1246w\"\n        sizes=\"(max-width: 1200px) 100vw, 1200px\"\n      />\n    </span>\n  </span>\n  \n  </a>\n    \nThis graph is great! It's almost a history of my life. Looking back at it, I can see when I made friendships and how they were developing. This is probably my favorite visualization!</p>\n<p>After that we have a word cloud and a tree for visualizing conversations, but it's a little too personal for me to post online.</p>\n<p>Later in the Jupyter notebook we find a chart that shows us our top words over the years, accounting for the relative frequency in other years:\n\n  <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/sms-analysis-4-2b106777dbb8e7dc4d57fbbc4ca4c12f-f77ec.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n  \n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; ; max-width: 756px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 126.71957671957672%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAYAAAAxFw7TAAAACXBIWXMAABYlAAAWJQFJUiTwAAAEl0lEQVQ4y01VSXbbVhDkibLNDbzJNi8HyBW8yxVyiKyTTXwIPYkWxRkgQAwkZo7gKM5Ed6o+bTnaCAQ+uqurqgu15WKhy+VS4zjSer2ujuNq4/VVu92uPj09ab/f19Vqpc1mU3u9nvDa9319xZkoiuR4PGiSJPr8/KxhGGptPp9rWZZa5Lm2220dDj3tdjqmEF+yLFsPh4NYliW8xtlbEAQXnK3iONb7/S4LgGo23xT3tYaDpqPtOMq/BIeiKNYFUPs4sN6siVyC0UgulwvP/nG73T5VVeWzQKPRqJ5f6myq2+1Wa2mWGsjjcaSX69WMPh6NdDqdqTNwdIYJHMeRZVnK+36vg8Egm85m3vl8Po3GY/NsMpno6XQyk9byotAsTZXwKxFTPIrGWi5LGbqughKOe9/t97fj8SiOM9ANkAjOhkBt27ZuNhsFYsN1DR00QqcBHqzXa83BJWmAAOoOhzIHP0StKmasdqtV2ZZdAREaDrXV7gjfOwMhxa15nqdxFJmC8/nCIKRaaZoKr0H4GVz9u95s/gGSjW1bWhSFYGQjwgCAFoul8rcpSEGIgAXLco1CifpoUhQTmc1m5GWb5flv+93+F4w0JnLcq67Xi3Jc2ms2m/8oyC5UlmO+NRra6XR1Al5ZzHFdKVdrDdCU4uwhig37EDkKCMGAFnP/o6AZGQh9P+AoxkJjcArlxIUoUJSinJM0PWPiO1HR0OQwDAPlGYpy+s4h+YLCUkymFZivksSglTwvtNVsCkVig+12pwIlW+22QF1oJFgCF8vQ0ff3d73CcsY26CB5lqkLctkJowp9iEIyxDgg/HI8HD6j2a/g8G8WxxahcSIxFuDbFAqzPwoSIR4e8aCPgkWG4igkeNEUxO+bO3S/DD3vLyjfJjV4p4IjeMasKkf9GHk6nRKNneXZT4D+J7u0Ws2qj7GJPo4TTbNcIwhHi7Rb7ep4OBpEnjcEWkt3/xcFo6LTCEhyLjr5goop4UsHYYH0gSj9OwreoCZMbWFDxvShMECW5cOD2KJHQY4FzoTCnE5n/mdxetCM/BgLo0OczWbL3TXXHJF0Ma32EOWjoFk92OD166sgLSSGx+ovdYGdcO+riTCOxf/LZWliyrIHBpXv+fqCHKQjLpcztwqijELt97oaoBu7MDlCoGJo8Hq/fzfZWGLxWQRpYzx7Op50BDdwIQBIb6CLO21U5toFMCkdD3GUsUSk3IhVudIe1ovi0W8dhC+jjtd8l4tB/tnM2IZd6KXGW9OsFwXhQW4NLcGoouKTyZQNBQUFqCs6mzt/Bu8U84ND7jKjn13ZhfAdjBWGj0Ys6rgOtikRGp8CsSHfwxqKBX4ZceTwR8FWyyBjQaqMvDMFyZ0HvoLAFxoeBbnXW9wvEb43BgnOmm/KB0IPYUCEUBnRlSlCQEOg5DeFCOkz7i95RcDesMe/i8rP2F13UkxoowpbpC/4rnCabyr3lBwRIdbLJA659TAer7mKJRqQQ+RfCUQznL1yOqY6C+12u4dtOOIjXEuTGLQKclAfYjnGa7QKx+UngLHPrxtXbzQKhT5mJn7/6v0HZukYUPc6xPwAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"words over the years\"\n        title=\"\"\n        src=\"/static/sms-analysis-4-2b106777dbb8e7dc4d57fbbc4ca4c12f-f77ec.png\"\n        srcset=\"/static/sms-analysis-4-2b106777dbb8e7dc4d57fbbc4ca4c12f-f6d30.png 300w,\n/static/sms-analysis-4-2b106777dbb8e7dc4d57fbbc4ca4c12f-3b26c.png 600w,\n/static/sms-analysis-4-2b106777dbb8e7dc4d57fbbc4ca4c12f-f77ec.png 756w\"\n        sizes=\"(max-width: 756px) 100vw, 756px\"\n      />\n    </span>\n  </span>\n  \n  </a>\n    \nHere we see me moving to the US, but also how I was interacting. The blacked out sections are names of people. Fascinating who I was talking about ðŸ˜‰.</p>\n<p>This project has not seen any attention in the last months, but there is still so much more to explore.</p>\n<h1 id=\"how-it-works\"><a href=\"#how-it-works\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How it works</h1>\n<p>We start by loading the data into our notebook</p>\n<div class=\"gatsby-highlight\" data-language=\"python\">\n      <pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> iphone_connector\niphone_connector<span class=\"token punctuation\">.</span>initialize<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nfully_merged_messages_df<span class=\"token punctuation\">,</span> address_book_df <span class=\"token operator\">=</span> iphone_connector<span class=\"token punctuation\">.</span>get_cleaned_fully_merged_messages<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n      </div>\n<p>The connector does all the heavy lifting. The iphone_connector accesses the latest iPhone backup and reads all text messages from its database. The facebook_connector uses fbchat_archive_parser to read a user's archive of chat messages. Now we have fully_merged_messages_df and address_book_df, both of which are <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\">Pandas Dataframes</a>.</p>\n<p>Once we have the messages we can create the heatmap. First we group our messages by month:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\">\n      <pre class=\"language-python\"><code class=\"language-python\">month_year_messages <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">[</span><span class=\"token string\">'date'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nmonth_year_messages<span class=\"token punctuation\">[</span><span class=\"token string\">'year'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> month_year_messages<span class=\"token punctuation\">.</span><span class=\"token builtin\">apply</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> row<span class=\"token punctuation\">:</span> row<span class=\"token punctuation\">.</span>date<span class=\"token punctuation\">.</span>year<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nmonth_year_messages<span class=\"token punctuation\">[</span><span class=\"token string\">'month'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> month_year_messages<span class=\"token punctuation\">.</span><span class=\"token builtin\">apply</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> row<span class=\"token punctuation\">:</span> row<span class=\"token punctuation\">.</span>date<span class=\"token punctuation\">.</span>month<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></code></pre>\n      </div>\n<p>Then we create a pivot table that contains the number of messages per month.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\">\n      <pre class=\"language-python\"><code class=\"language-python\">month_year_messages_pivot <span class=\"token operator\">=</span> month_year_messages<span class=\"token punctuation\">.</span>pivot_table<span class=\"token punctuation\">(</span>index<span class=\"token operator\">=</span><span class=\"token string\">'year'</span><span class=\"token punctuation\">,</span>columns<span class=\"token operator\">=</span><span class=\"token string\">'month'</span><span class=\"token punctuation\">,</span>aggfunc<span class=\"token operator\">=</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">,</span> dropna<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\nmonth_year_messages_pivot <span class=\"token operator\">=</span> month_year_messages_pivot<span class=\"token punctuation\">[</span>month_year_messages_pivot<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span>axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">12</span><span class=\"token punctuation\">]</span></code></pre>\n      </div>\n<p>After that we feed the data into the plotting tool.</p>\n<p>Next we want to create the barchart to see who we have messaged the most in the past. We group the messages by message partner and count how many you sent and received.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\">\n      <pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">get_message_counts</span><span class=\"token punctuation\">(</span>dataframe<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> pd<span class=\"token punctuation\">.</span>Series<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span>\n      <span class=\"token string\">'Texts sent'</span><span class=\"token punctuation\">:</span> dataframe<span class=\"token punctuation\">[</span>dataframe<span class=\"token punctuation\">.</span>is_from_me <span class=\"token operator\">==</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n      <span class=\"token string\">'Texts received'</span><span class=\"token punctuation\">:</span> dataframe<span class=\"token punctuation\">[</span>dataframe<span class=\"token punctuation\">.</span>is_from_me <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n      <span class=\"token string\">'Texts exchanged'</span><span class=\"token punctuation\">:</span> dataframe<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\nmessages_grouped <span class=\"token operator\">=</span> fully_merged_messages_df<span class=\"token punctuation\">.</span>groupby<span class=\"token punctuation\">(</span><span class=\"token string\">'full_name'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">apply</span><span class=\"token punctuation\">(</span>get_message_counts<span class=\"token punctuation\">)</span>\nmessages_grouped <span class=\"token operator\">=</span> messages_grouped<span class=\"token punctuation\">.</span>sort_values<span class=\"token punctuation\">(</span>by<span class=\"token operator\">=</span><span class=\"token string\">'Texts exchanged'</span><span class=\"token punctuation\">,</span> ascending<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></code></pre>\n      </div>\n<p>By combining the ideas from the two charts above we can create the chart that shows our interactions with friends over time. We select our TOP_N friends and count how many messages we sent them each month.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\">\n      <pre class=\"language-python\"><code class=\"language-python\">sliced_df <span class=\"token operator\">=</span> fully_merged_messages_df<span class=\"token punctuation\">[</span>fully_merged_messages_df<span class=\"token punctuation\">.</span>full_name<span class=\"token punctuation\">.</span>isin<span class=\"token punctuation\">(</span>messages_grouped<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span>TOP_N<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>index<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\ngrouped_by_month <span class=\"token operator\">=</span> sliced_df<span class=\"token punctuation\">.</span>groupby<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    sliced_df<span class=\"token punctuation\">.</span><span class=\"token builtin\">apply</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> x<span class=\"token punctuation\">:</span> x<span class=\"token punctuation\">.</span>date<span class=\"token punctuation\">.</span>strftime<span class=\"token punctuation\">(</span><span class=\"token string\">'%Y/%m'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'full_name'</span><span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">'text'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to_frame<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n      </div>\n<p>After that we graph it using <a href=\"https://github.com/d3/d3\">D3.js</a>.</p>\n<p>The final chart - our most used words per year - requires us to dive into NLP. We'll be using Tfidf (term-frequency times inverse document-frequency) to calculate the relevance of each word.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\">\n      <pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> TfidfVectorizer\n<span class=\"token keyword\">from</span> nltk <span class=\"token keyword\">import</span> tokenize\nvectorizer <span class=\"token operator\">=</span> TfidfVectorizer<span class=\"token punctuation\">(</span>preprocessor<span class=\"token operator\">=</span>clean_text<span class=\"token punctuation\">,</span> tokenizer<span class=\"token operator\">=</span>tokenize<span class=\"token punctuation\">.</span>WordPunctTokenizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>tokenize<span class=\"token punctuation\">,</span> ngram_range<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> max_df<span class=\"token operator\">=</span><span class=\"token number\">.9</span><span class=\"token punctuation\">,</span> max_features<span class=\"token operator\">=</span><span class=\"token number\">50000</span><span class=\"token punctuation\">)</span>\ngrouped_by_name <span class=\"token operator\">=</span> fully_merged_messages_df<span class=\"token punctuation\">[</span>fully_merged_messages_df<span class=\"token punctuation\">.</span>is_from_me <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>groupby<span class=\"token punctuation\">(</span><span class=\"token string\">'full_name'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">'text'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">apply</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> x<span class=\"token punctuation\">:</span> <span class=\"token string\">' '</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to_frame<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nvectorizer<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>grouped_by_name<span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">)</span></code></pre>\n      </div>\n<p>The above code identifies the most relevant word for a certain chat partner, but important is, that it trains our TfidfVectorizer. We can now use it to find the prominence of a word in a given year.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\">\n      <pre class=\"language-python\"><code class=\"language-python\">grouper <span class=\"token operator\">=</span> slice_of_texts_df<span class=\"token punctuation\">.</span>date<span class=\"token punctuation\">.</span><span class=\"token builtin\">apply</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> x<span class=\"token punctuation\">:</span> x<span class=\"token punctuation\">.</span>year<span class=\"token punctuation\">)</span>\ngrouped_by_year <span class=\"token operator\">=</span> slice_of_texts_df<span class=\"token punctuation\">.</span>groupby<span class=\"token punctuation\">(</span>grouper<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">apply</span><span class=\"token punctuation\">(</span>\n  <span class=\"token keyword\">lambda</span> row<span class=\"token punctuation\">:</span> pd<span class=\"token punctuation\">.</span>Series<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">'count'</span><span class=\"token punctuation\">:</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>row<span class=\"token punctuation\">.</span>date<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'text'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">' '</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>row<span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\ngrouped_by_year_tfidf <span class=\"token operator\">=</span> vectorizer<span class=\"token punctuation\">.</span>transform<span class=\"token punctuation\">(</span>grouped_by_year<span class=\"token punctuation\">[</span><span class=\"token string\">'text'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre>\n      </div>\n<p>Now we need to find the most important word in comparison to other years.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\">\n      <pre class=\"language-python\"><code class=\"language-python\">sorted_indices <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>tfidf_this_year <span class=\"token operator\">-</span> tfidf_other_years<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>argsort<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\ndf <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span>this_year<span class=\"token punctuation\">:</span> word_list<span class=\"token punctuation\">.</span>iloc<span class=\"token punctuation\">[</span>sorted_indices<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>top_n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span></code></pre>\n      </div>\n<h1 id=\"my-contribution\"><a href=\"#my-contribution\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>My contribution</h1>\n<p>After the presentation I was super excited about the idea and wanted to try it out. When I first tried it, it only supported iPhones. I decided to add a Facebook Chat connector. It still has minor issues, such as an ID not being properly resolved, but it works (in fact all those charts above use my FB data).</p>\n<p>I have also been looking into clustering. Having the word vectors would allow us to cluster our text/chat partners and see who we interact with similarly. I am still working on it, but below is how a first draft looks (names purposefully omitted).\n\n  <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/sms-analysis-5-f645965c9d3ae51c090702637ea54822-d6fae.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n  \n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; ; max-width: 963px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 52.232606438213914%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAABrElEQVQoz1VSW3LbMAzU/W/R/mcmPUP9kyYdN2kVR7JEUXyAkmyfYbsA1ST94AAChcXugs3tdsO6rti21WLNN2yMyyLIS+T35b+7z/Fz7/V6RaNJjBFZCnLOSCnCxwlpnhDdG+/mvZ6QGP/l7zXLxWIpBY1Oylpks/40hQExB9Y4xJ+RZwfhj/YPa6J1BcgVVEQwJw5nfV1WNBfK8XFEmnprmskuJF8BCVaZU3rwSH6oA2SBD6OBaD7OnRFZ100lbxhHnSA2TWmHoHLyzoxRpfJOj5QKZnkMNiBJgvMdezOahTSffw0EDfBTPXGOSDGxIX0A7f7piSmYZPOXqnSIel5Yby6XDcPgcTwOcCcuox0RXgYEAguB86QsIkJRlsq6KlFQXZ4EghJYWZdlqUvRDbWtw4mn7+gfY/7eQr4cIN+eIC+qgLU3h9B7RB/4pIoBC/09dw/w/REL92HPpq5cTHb/mws6tMh3PyBfD0j3j8hP9IeD0vMZw6tD+2e0wdEsKZQ7mmS1rz6bff02lXmYuL2OnriAmeyUofvZw71WBSda433cGWbzUH1eKfkvn+fZ0OKV03QAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"clustered contacts\"\n        title=\"\"\n        src=\"/static/sms-analysis-5-f645965c9d3ae51c090702637ea54822-d6fae.png\"\n        srcset=\"/static/sms-analysis-5-f645965c9d3ae51c090702637ea54822-74412.png 300w,\n/static/sms-analysis-5-f645965c9d3ae51c090702637ea54822-4a056.png 600w,\n/static/sms-analysis-5-f645965c9d3ae51c090702637ea54822-d6fae.png 963w\"\n        sizes=\"(max-width: 963px) 100vw, 963px\"\n      />\n    </span>\n  </span>\n  \n  </a>\n    </p>\n<h1 id=\"check-it-out\"><a href=\"#check-it-out\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Check it out</h1>\n<p>I'd recommend anybody to check out the repository. Michael has been awesome in reviewing my code and gave me so many helpful tips. You can find the repository <a href=\"https://github.com/mdezube/sms-analysis\">here</a>.</p>","timeToRead":5,"excerpt":"About a year ago I attended a  Meetup on D3.js . The first talk by  Michael Dezube  was on using machine learning to analyze text messagesâ€¦","frontmatter":{"title":"Messaging Analysis and Visualization","cover":{"childImageSharp":{"sizes":{"tracedSVG":"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' width='400' height='266' viewBox='0 0 400 266' version='1'%3e%3cpath d='M0 24v24h9l6-1c-3 0-3-2 0-2l5-1 8 1 9 1c3 0 3 0-1 1l-2 1c5 0 1 2-6 2-18 2-21 2-22 1 0-1-1-2-3-2-3 0-3 0-3 4v4h10l-2 1H3c-3 1-3 1-3 4l1 3v1c-2 0-1 2 1 2v1c-2 0-2 1-2 9v8h6l8 1c1 1 1 1-1 1h-1v1l-5 1-2-1c1-2 0-2-2-2-3 0-3 0-3 4l1 3c2-2 16-2 16-1 1 1 0 1-9 2-7 0-8 0-8 2l3 1 3-1h16c1 1 0 1-1 1-2 0-3 0-2 2 0 2-7 2-16 0-2 0-3 0-3 2 0 1 1 2 12 1h13c1-1 1-1 2 1l1 2c1 1 0 1-1 1s-2 0-1 1l2 1c2 0 2 0 0 1-1 2-1 2 2 2 4 0 6 2 4 4v1c4 0 5 1 7 3 1 2 2 3 1 4s0 1 1 2l3 1 1 1c3-1 5 3 2 4-2 1-1 4 2 3 3 0 6 2 5 4l1 2c2-1 2-1 1 1l1 2 3 2 1 2 1 2 1 1 1 1 3 3 3 4 1 1 3 4 4 4c2 0 3 3 2 4-2 1-1 2 1 2l2 1 2 2 2 1 3 4 2 3 2 2 2 2-1 1h-1c0 1 1 2 3 2l2 1 3 5 3 4 2 1 1 3 2 2c2 0 4 2 4 6 1 3 2 3 3 3 2-1 2 0 2 3s0 3 3 4 4 2 4 5l2 3c3 0 6 3 6 6l2 1 3 3 3 3 1 1 2 3 2 2c0 2 4 3 9 3 2 0 2 0 1 1s-1 1 2 1c2 0 2 0 1 1-4 1 1 2 8 1 5 0 7 0 6 1l-10 1-11 2-7 1-11 1c-5 2-4 2 16 1 28 0 41-1 36-2l-4-2c0-2 8-2 9 0 3 3 35-1 33-4l2-1c4 2 10 2 12 1l7-2h2l-1 1c6 0-1 2-14 4-17 2-20 4-5 3 13-1 10 1-3 2-7 1-5 1 11 1l23-1h3l69 1h68V0H0v24m60-5a1568 1568 0 0 1-1 4l-4 1a130 130 0 0 0-22 1l2 1c1 1-6 2-11 2l-3 1h14c11 0 12 0 8 1-12 2-15 3-7 2h15l19-2-2-1-3-1 4-1c3 0 4 1 4 2-1 2 2 2 4 1l1-1 6-1c4-1 4-2 2-2s-3 0-2-1l-4-1c-15-2-16-2-16-1h-1l-2-2-1-2m57 5c-4 1-8 3-5 3v2c0 2 0 2 2 1s5-1 5 1c-1 1 0 1 1 1 2 0 2 0 1 1-2 1 3 1 8-1l2 1c-2 2 12 1 15-1 1-2 2-2 4-2h4c0-2 4-1 3 0 0 1 2 2 8 2h14l-4-1c-1 0-2 0-1-1h16c10 0 13-1 7-1l-4-1h14a209 209 0 0 1 28-2l5 1h3c7-2-9-3-47-3a5659 5659 0 0 1-79 0m146 33l-2 10-1 6-2-7-3-8-16-1-9 1-2 4v3l-2-4c-1-5-1-5-3-4h-17l-7 1h-5c0-2-9-2-25-2-4 0-4 0-5 5l1 9-1 2c-5 0-7 0-5 1l4 1c4 0 5 0 5 2l-3 1c-3 0-3 0-2 1h4c1-1 2-1 2 1 1 1 0 1-6 2-2 1-1 1 1 1l4 1 2 1c4 1 4 3 0 3-2 0-3 0-2 1l12 1c13 0 9 2-5 2l-7 1 1 1 13 1 4 1-7 1-4 1c0 2 8 2 18 1h35c0 2-3 4-7 4l-5 1h5c5 0 5 0 4 2l-3 2v-1l-3-1c-2 0-3 0-2 1l2 1-2 1-25 1c-2 0-4 5-1 5l1 1c-1 2 4 5 7 5l1 2 2 1c2 0 2 0 1-1s-1-1 2-1l3-1h-4c-4 0-6-1-2-2 3-2 9-1 8 1-2 2 1 1 3-1 2-1 10-2 9 0l3 2c3 1 3 1 1 1h-7l-3 1h12c3 0 4 0 4 2s1 3 2 1v-1l-1-2 2 1 4 1 13-2c10-1 10-1 12-7l1-2 4 6c3 5 3 5 1 5l-2-1-1-2v2c0 2-5 3-19 4a70 70 0 0 0-14 1h-7c-5 0-5 0-3 1 4 2 3 3-3 3l-5 1 2 1 9-1c5 0 7 0 6-1-6 0-6-2 0-2l14-1 11-1c1 1 1 1-1 1a188 188 0 0 0-16 3c6 0 6 2 0 2-4 0-4 0-2 2l5 1 9-1 6-1 2-2c4-1 3-2-2-1l-3-1 2-1h9c4 0 4 2 1 2l-3 1h13c4-2 4-2 1-2s-5-2-1-2l1-1 2-1 2-2 1-2 1 1c-1 2 1 1 2 0l3-2 2-1h1c2 1 6-1 7-3l1 2 1 5h13c15 0 16-1 16-8l-1-4 2 4 2 5c0 3 2 3 8 1l11-2c4 0 6-1 7-3l3-2v3l1 3h11c12 0 12-1 15-3 2-3 2-3 1-7l-4-10c-1-7-1-7-16-7l-11-1-1 3c-1 3-2 4 0 7l1 5c0 2-5-6-6-10 0-2-3-5-6-4l-11 1c-8 1-9 2-10 3-1 2-1 2-2 0-1-3-4-4-5-2h-18c-3-1-7 7-5 10v1l-4-5-4-5v-1l-1-2c-4-1-14-2-11 0 2 1 1 1-8 5h-5l-2-1 1 2-1 4-1 1-1-2-3-6-2-5-7-1c-10 0-10-1-1-1h17c7 0 10-1 10-2l-17-1c-14 0-17 0-17 2l-4 1c-2 0-3 0-2-1h2l3-1-17-1h-17l2-1 11-1h10l1-4 1-8v-1l2 6c0 6 1 7 5 7l12-1c8 0 9-1 11-3s3-2 5-1l4 1 7 2c10 4 10 3 13-12 2-11 2-11-9-14l-11-3c-5-1-5-1-6 2m71 5l-10 1-16-2h-10l-1 3v18c1 0 2 0 2 2 0 3 2 4 14 4 13 1 15 0 15-7v-3l2 2 1 4 2 4c1 2 2 2 7 0l6-2 4-1 3-1 2-1c5-2 5-3 1-9-9-16-7-15-22-12m-126 2c-3 4 3 10 9 9 3-1 4-1 4-4 0-6-8-9-13-5m-93 25v1l1 1-2 1-2 2-3 1-2 1c0 1-5 2-6 1l-3-1c-1 0-2 0-1 1 2 0 3 3 1 3l-2 1-1 1-3 2-3 1-2 1c-1 1-3 2-2 0l-2-1c-2 1-2 2 0 5s7 4 6 2c-1-1 0-1 0 0l4-2 3-2 2-1 5-2 5-3 3-1 2-1 3-2 3-1c0-1 1-2 2-1l1-1 2-1v-1l-2-3-2-2-1 2h-1c-1-2-3-3-3-1m32 0l-10 6-68 36 17 19 4-2c3-3 14-8 32-18l44-24-8-10-6-7h-5m100 12c0 4 0 4-4 1-3-3-4-4-5-2-1 1 3 10 4 10v-2c-2-4-1-4 2-1 4 3 5 3 5-2v-3l2 3c1 4 4 5 4 2l-2-5c-3-5-6-6-6-1m61 2v5c2 5 4 6 10 5 3-1 3-6-1-9-2-2-8-3-9-1m-22 48l-8 2a216 216 0 0 0-37 2l-11 1 8 3a66 66 0 0 1 7 0c-4-1-2-2 4-2l6-1v-1l1 1 1 1 14-1-3 1-3 1-7 1c-8 0-11 1-8 3 2 1 1 2-2 2-4 1-4 1-3 3h4l9-1c7 0 8-2 2-2l-4-1h4l4-1 7-3v1h1l8-1c6 0 7-2 1-2-7 0-7-2 0-2l14-1v-1c-1-2-6-3-9-2M1 165v6l4-1 6-1 10-2c11-1 13-2 3-1-7 0-8 0-7-1l8-1 7-1-7-1-9-1c1-2 0-2-7-2H1v6m285 28c0 1-3 2-4 0h-10l1 1c1 1 0 1-1 1-6 0-5 2 2 2l8 1c1 1-1 2-7 3-9 1-15 3-7 3l14-1v2c-2 1-2 1-1 2h8l3-1-3-1-1-1-2-1c-4 0 1-1 7-2h8l5-1c3 0 5-1 5-2 0-2-3-2-6-1l-5 1-8 2-2-1c1-1 0-1-1-2-3-1 1-2 11-3l4-2-2-1c-3 2-14 2-14 1-1-2-3-1-2 1M7 199H1v12l1 12v-3c0-3 3-4 13-5l6-1c0-2 2-3 5-3l3-1-11-1H9l5-1h3l-3-1c-3-1-3-1 2-1 7-1 5-3-3-3-6 0-5-1 2-2l3-2-3-1-8 1m1 19l-4 2 2 2c2 1 2 1 0 1-5 0-5 0-5 22v21h13l13-1h2c0 2 3 1 3 0l2-2v2c-1 1-1 1 4 1 4 0 5 0 5-2 1-2 2-2 5-2l6-1H42c-9 0-14 0-12-1l8-1c8 0 8-2 1-3-7 0-8-1-4-3 2-1 2-1-3-1-4 0-5 0-5-2h-3l-6-1c-3-2 4-2 15 0h7l2-1 1-1-7-1c-12 0-19-1-20-2-2-1-2-4-1-4l-2-2c-4-2-4-2 0-4l6-4c1-2 3-3 5-3s5-2 3-2l-4-1c-1-1-2-2-6-1-8 0-8-1-1-3 5-1 5-1 3-3-3-1-7-1-11 1m216 15l-1 1c-1-1-8 1-8 3 1 1-7 3-20 3-3 0-3 0-3 3l1 3-1 2c-3 1-3 1-1 1 3 0-1 2-7 2-9 2-11 2-11 4-1 1 2 3 3 1s20-5 20-3l-6 2-6 1c0 1 8 1 14-2l11-2 8-2 1 1-2 2-3 1 16-1c1-1 0-1-2-1-3 0-3 0 3-2l8-1h2l-2 2c-3 1-1 2 3 1l15-1 21-1 7-1 9-3c5-2 2-3-5 0h-5l-3-1c-2 0-2 0-1 1s1 1-1 1l-3 1h-6l2-1c4-2 3-4-1-3-4 0-4 0-4-2 1-2-2-3-6-2l-1 1-1 1c-2 1-2 1 1 1l3-1 1-1 1 2-5 1-9 1c-5 1-8 1-6-1l7-2c5 0 5 0 2-1l-3-1 5-1 7-3-3-1c-5 1-8 0-8-2v-1l-1 1-2 2a335 335 0 0 0-20 0l-1-2h-3' fill='lightgray' fill-rule='evenodd'/%3e%3c/svg%3e","aspectRatio":2.272727272727273,"src":"/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-4a2eb.jpg","srcSet":"/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-8495f.jpg 200w,\n/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-ffa71.jpg 400w,\n/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-4a2eb.jpg 800w,\n/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-66e1b.jpg 1200w,\n/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-fe773.jpg 1600w,\n/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-40ca2.jpg 2400w,\n/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-7ed14.jpg 3967w","srcWebp":"/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-dd2a8.webp","srcSetWebp":"/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-c4602.webp 200w,\n/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-54b15.webp 400w,\n/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-dd2a8.webp 800w,\n/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-86411.webp 1200w,\n/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-23d56.webp 1600w,\n/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-c36a5.webp 2400w,\n/static/social-media-fa348c08f83e2d7a89446a9b7bf61443-0bd0d.webp 3967w","sizes":"(max-width: 800px) 100vw, 800px"}}},"date":"02/14/2018","category":"Machine Learning","tags":["Programming","NLP","Visualization","Machine Learning","Past Project"]},"fields":{"nextTitle":"Lamport Clocks","nextSlug":"/articles/lamport-clocks","prevTitle":"Trying to build a good web app","prevSlug":"/articles/trying-to-build-a-good-web-app","slug":"/articles/messaging-analysis-and-visualization"}}},"pageContext":{"jsonName":"articles-messaging-analysis-and-visualization-938","internalComponentName":"ComponentArticlesMessagingAnalysisAndVisualization","path":"/articles/messaging-analysis-and-visualization","component":"/Users/nigel/Documents/Projects/personal-website/src/templates/post.jsx","componentChunkName":"component---src-templates-post-jsx","context":{"slug":"/articles/messaging-analysis-and-visualization"},"updatedAt":1530123728593,"pluginCreator___NODE":"Plugin default-site-plugin","pluginCreatorId":"Plugin default-site-plugin","componentPath":"/Users/nigel/Documents/Projects/personal-website/src/templates/post.jsx","slug":"/articles/messaging-analysis-and-visualization"}}