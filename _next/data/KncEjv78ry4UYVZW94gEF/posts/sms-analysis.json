{"pageProps":{"__typename":"Query","moreStories":[{"__typename":"Post","author":{"__typename":"Author","name":"Nigel Schuster","picture":{"__typename":"Asset","url":"https://media.graphcms.com/resize=fit:crop,height:100,width:100/1bF1qLxsSbElgNK5x6t6"},"slug":"nigel-schuster"},"coverImage":null,"excerpt":"Using generic types like strings made me make mistakes on several occasion. There is a better way.","slug":"avoid-generic-types","title":"Avoiding generic types for safer code"},{"__typename":"Post","author":{"__typename":"Author","name":"Nigel Schuster","picture":{"__typename":"Asset","url":"https://media.graphcms.com/resize=fit:crop,height:100,width:100/1bF1qLxsSbElgNK5x6t6"},"slug":"nigel-schuster"},"coverImage":null,"excerpt":"A good CSP can be a line of defence against XSS. It isn't trivial to get it right though.","slug":"how-to-csp","title":"How to CSP"}],"post":{"__typename":"Post","author":{"__typename":"Author","name":"Nigel Schuster","picture":{"__typename":"Asset","url":"https://media.graphcms.com/resize=fit:crop,height:100,width:100/1bF1qLxsSbElgNK5x6t6"},"slug":"nigel-schuster"},"content":"About a year ago I attended a [Meetup on D3.js](https://www.meetup.com/it-IT/NYC-D3-JS/events/236673854/). The first talk by [Michael Dezube](https://github.com/mdezube) was on using machine learning to analyze text messages and visualize the results. Today I want to walk through all the awesomeness we have in this project.\n\n# Demo\nThe first chart we see is this\n![texting per month](https://media.graphcms.com/BOw9T1zYQ4eUKUCZTK9d)\n\n\nHere it shows us our messaging habits over the years. Fascinating how it varies! But let's dive deeper. The important question is: Who did we message? We can display the top N people:\n![top people messaged with](https://media.graphcms.com/kr0XNn7cQY21lkD2C1te)\n\nBut who did we message and when?\n![people messaged over time](https://media.graphcms.com/pkGf8kxTnevn7OiELmvw)\n\nThis graph is great! It's almost a history of my life. Looking back at it, I can see when I made friendships and how they were developing. This is probably my favorite visualization!\n\nAfter that we have a word cloud and a tree for visualizing conversations, but it's a little too personal for me to post online.\n\nLater in the Jupyter notebook we find a chart that shows us our top words over the years, accounting for the relative frequency in other years:\n![words over the years](https://media.graphcms.com/bBSQzlOjSneomJ3MtO2C)\n\nHere we see me moving to the US, but also how I was interacting. The blacked out sections are names of people. Fascinating who I was talking about ðŸ˜‰.\n\nThis project has not seen any attention in the last months, but there is still so much more to explore.\n\n# How it works\nWe start by loading the data into our notebook\n```python\nimport iphone_connector\niphone_connector.initialize()\nfully_merged_messages_df, address_book_df = iphone_connector.get_cleaned_fully_merged_messages()\n```\nThe connector does all the heavy lifting. The iphone\\_connector accesses the latest iPhone backup and reads all text messages from its database. The facebook\\_connector uses fbchat\\_archive\\_parser to read a user's archive of chat messages. Now we have fully\\_merged\\_messages\\_df and address\\_book\\_df, both of which are [Pandas Dataframes](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html).\n\nOnce we have the messages we can create the heatmap. First we group our messages by month:\n```python\nmonth_year_messages = pd.DataFrame(df['date'])\nmonth_year_messages['year'] = month_year_messages.apply(lambda row: row.date.year, axis=1)\nmonth_year_messages['month'] = month_year_messages.apply(lambda row: row.date.month, axis=1)\n```\nThen we create a pivot table that contains the number of messages per month.\n```python\nmonth_year_messages_pivot = month_year_messages.pivot_table(index='year',columns='month',aggfunc=len, dropna=True)\nmonth_year_messages_pivot = month_year_messages_pivot[month_year_messages_pivot.count(axis=1) == 12]\n```\nAfter that we feed the data into the plotting tool.\n\nNext we want to create the barchart to see who we have messaged the most in the past. We group the messages by message partner and count how many you sent and received.\n```python\ndef get_message_counts(dataframe):\n    return pd.Series({\n      'Texts sent': dataframe[dataframe.is_from_me == 1].shape[0],\n      'Texts received': dataframe[dataframe.is_from_me == 0].shape[0],\n      'Texts exchanged': dataframe.shape[0]\n    })\nmessages_grouped = fully_merged_messages_df.groupby('full_name').apply(get_message_counts)\nmessages_grouped = messages_grouped.sort_values(by='Texts exchanged', ascending=False)\n```\nBy combining the ideas from the two charts above we can create the chart that shows our interactions with friends over time. We select our TOP\\_N friends and count how many messages we sent them each month.\n```python\nsliced_df = fully_merged_messages_df[fully_merged_messages_df.full_name.isin(messages_grouped.head(TOP_N).index)]\ngrouped_by_month = sliced_df.groupby([\n    sliced_df.apply(lambda x: x.date.strftime('%Y/%m'), axis=1),\n    'full_name']\n)['text'].count().to_frame()\n```\nAfter that we graph it using [D3.js](https://github.com/d3/d3).\n\nThe final chart - our most used words per year - requires us to dive into NLP. We'll be using Tfidf (term-frequency times inverse document-frequency) to calculate the relevance of each word.\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk import tokenize\nvectorizer = TfidfVectorizer(preprocessor=clean_text, tokenizer=tokenize.WordPunctTokenizer().tokenize, ngram_range=(1, 2), max_df=.9, max_features=50000)\ngrouped_by_name = fully_merged_messages_df[fully_merged_messages_df.is_from_me == 0].groupby('full_name')['text'].apply(lambda x: ' '.join(x)).to_frame()\nvectorizer.fit(grouped_by_name.text)\n```\nThe above code identifies the most relevant word for a certain chat partner, but important is, that it trains our TfidfVectorizer. We can now use it to find the prominence of a word in a given year.\n```python\ngrouper = slice_of_texts_df.date.apply(lambda x: x.year)\ngrouped_by_year = slice_of_texts_df.groupby(grouper).apply(\n  lambda row: pd.Series({'count': len(row.date), 'text': ' '.join(row.text)})\n)\ngrouped_by_year_tfidf = vectorizer.transform(grouped_by_year['text'])\n```\nNow we need to find the most important word in comparison to other years.\n```python\nsorted_indices = (tfidf_this_year - tfidf_other_years).argsort()[::-1]\ndf = pd.DataFrame({this_year: word_list.iloc[sorted_indices[:top_n]]})\n```\n\n# My contribution\nAfter the presentation I was super excited about the idea and wanted to try it out. When I first tried it, it only supported iPhones. I decided to add a Facebook Chat connector. It still has minor issues, such as an ID not being properly resolved, but it works (in fact all those charts above use my FB data).\n\nI have also been looking into clustering. Having the word vectors would allow us to cluster our text/chat partners and see who we interact with similarly. I am still working on it, but below is how a first draft looks (names purposefully omitted).\n![clustered contacts](https://media.graphcms.com/Zn1uK6zpScGhzgdXFbvw)\n\n# Check it out\nI'd recommend anybody to check out the repository. Michael has been awesome in reviewing my code and gave me so many helpful tips. You can find the repository [here](https://github.com/mdezube/sms-analysis).\n","coverImage":null,"date":"2018-02-14","ogImage":null,"slug":"sms-analysis","title":"Messaging Analysis and Visualization"}},"__N_SSG":true}